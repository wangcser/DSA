# 大数据问题

## 分析

大数据问题，表现为海量数据的存储和处理

- 海量：数据量大导致无法再短时间内迅速处理，或者无法一次性装入内存进行处理

解决方案可以分为两个方向：

- 使用空间利用率更好的数据结构和算法：
  - Bloom filter
  - Hash table
  - bitmap
  - heap
  - trie
- 使用能够简化问题规模的算法设计思路：
  - 分治算法
  - 分类，分阶段，分部导入和处理



基于上面的两种方案，我们可以总结出大数据问题中常用的处理思路：

- 分治 + [hash 统计 | 堆 、快速、归并排序]
- 双层桶
- Bloom filter 、bitmap
- trie
- 分布式处理：hadoop、mapreduce

## 方案一：分治映射 + 频率统计 + 归并排序

方法：分而治之/Hash映射 + Hash_map统计 + 堆/快速/归并排序

#### 给定**海量日志数据，提取出某日访问百度次数最多的那个IP**

处理的过程无非就是下面的三步：

- 先映射，确定一个映射关系
- 在统计，建表
- 最后排序，选择目标值

设计的方案如下：

1. 分而治之/hash映射：针对**数据太大，内存受限**，只能是：**把大文件化成(取模映射)小文件**，即16字方针：大而化小，各个击破，缩小规模，逐个解决
2. hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行**频率统计**。
3. 堆/快速排序：**统计完了之后，便进行排序(可采取堆排序)，得到次数最多的IP**。

> 具体而论，则是： “首先是这一天，并且是访问百度的日志中的IP取出来，逐个写入到一个大文件中。注意到IP是32位的，最多有个2^32个IP。同样可以采用映射的方法，比如%1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP（可以采用hash_map对那1000个文件中的所有IP进行频率统计，然后依次找出各个文件中频率最大的那个IP）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。”
>
> 这里 映射这一步很关键，保证了不同区域互不相关！

这里需要注意对 hash 的理解：

- hash 用于帮助计算机在有限内存的情况下，将大数据通过散列的方式均匀映射到有限的内存上去
- 数据还是那个数据，只是数据的表示发生了变化

#### 寻找热门查询， 300 万查询字符串中统计最热门的 10 个查询

> 搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门），请你统计最热门的10个查询串，要求使用的内存不能超过1G

方案：

- 方案 1
  - 由上面第1题，我们知道，数据大则划为小的，如如一亿个Ip求Top 10，可先%1000将ip分到1000个小文件中去，并保证一种ip只出现在一个文件中，再对每个小文件中的ip进行hashmap计数统计并按数量排序，最后归并或者最小堆依次处理每个小文件的top10以得到最后的解
- 方案 2
  - 虽然有一千万个Query，但是由于重复度比较高，因此事实上只有300万的Query，每个Query255Byte，因此我们可以考虑把他们都放进内存中去（300万个字符串假设没有重复，都是最大长度，那么最多占用内存3M*1K/4=0.75G。所以可以将所有字符串都存放在内存中进行处理），而现在只是需要一个合适的数据结构，在这里，HashTable绝对是我们优先的选择
  - 所以我们放弃分而治之/hash映射的步骤，直接上hash统计，然后排序。So，针对此类典型的TOP K问题，采取的对策往往是：hashmap + 堆

方案 2 详解：

1. hash_map统计：先对这批海量数据**预处理**。具体方法是：维护一个Key为Query字串，Value为该Query出现次数的HashTable，即hash_map(Query，Value)，每次读取一个Query，如果该字串不在Table中，那么加入该字串，并且将Value值设为1；如果该字串在Table中，那么将该字串的计数加一即可。最终我们在O(N)的时间复杂度内用Hash表完成了统计；
2. 堆排序：第二步、借助堆这个数据结构，找出Top K，时间复杂度为N‘logK。即借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。所以，我们最终的时间复杂度是：O（N） + N' * O（logK），（N为1000万，N’为300万）

> 堆排序的细节：小顶堆不是大顶堆
>
> 别忘了这篇文章中所述的堆排序思路：“维护k个元素的最小堆，即用容量为k的最小堆存储最先遍历到的k个数，并假设它们即是最大的k个数，建堆费时O（k），并调整堆(费时O（logk）)后，有k1>k2>...kmin（kmin设为小顶堆中最小元素）。继续遍历数列，每次遍历一个元素x，与堆顶元素比较，若x>kmin，则更新堆（x入堆，用时logk），否则不更新堆。这样下来，总费时O（k*logk+（n-k）*logk）=O（n*logk）。此方法得益于在堆中，查找等各项操作时间复杂度均为logk。

优化：tire 结构

-  当然，你也可以采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。

### 练习：分治 + 映射 + 统计 + 排序

#### 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词

> 内存大小限制为 1MB，对于 1GB 大小的文件，将其分为1024 份则每份为 1 MB，因此将其分为 5000 份进行处理

- 顺序读取文件，对于每个词 x 取 hash(x) % 5000，将单词 hash 的结果存入硬盘中对应的 5000 个小文件之中，每个文件 200K 大小、若有的文件超过了 1M 的限制，则可以对其继续分
- 对每个小文件，使用 trie 或者 hashmap 统计词频
- 使用堆排序或者归并排序，求出每个文件中出现频率最大的 100 个词，再将100 个词及其频率存入文件，得到 5000 个文件
- 将这 5000 个文件归并即可

#### 海量数据分布在100台电脑中，想个办法高效统计出这批数据的 TOP10

如果每个元素只出现在某一台机器中：

1. 堆排序：在每台电脑上求出 top 10，可以采用包含 10 个元素的堆完成

> TOP10小，用最大堆，TOP10大，用最小堆，比如求TOP10大，我们首先取前10个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整为最小堆。最后堆中的元素就是TOP10大

2. 求出每台电脑上的 top 10 后，将 1000 个 top 10 组合起来，共 1000 个数据，再求一遍 top 10 即可

如果某个元素出现在多个机器中：

1. 遍历所有数据，对其 hash 取模，使得同一个元素只出现在单独的一台电脑中，然后步骤同上

2. 暴力求解：直接统计每台电脑中各个元素的出现次数，然后将同一个元素在不同机器中出现的次数相加

#### 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序

方案1：

1. 映射：熟悉读取 10 个文件，按照 hash(query) % 10 的结果将其写入另外 10 个文件中，这样新生的文件每个大约 1 G，同时保证了重复数据都位于同一个文件中
2. 统计：找到一台 2G 内存的机器，依次用  hash(query, query_count) 统计其频率
3. 排序：将 pair 按照值排序，将排序结果输出，得到 10 个文件，最后对这 10 个文件进行归并

方案 2：trie

- 一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。

方案 3：分布式

- 与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。



#### 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

> 每个文件的大小为 5G×64 Byte = 320G，采用分治策略：分为 1000 个文件，每个 320 M

1. 映射：遍历文件 a，对每个 url 重新映射 hash(url) % 1000，根据结果将 url 存储于 1000 个小文件，每个小文件约 300M，遍历 b，同样的方法得到 1000 个小文件，此时，相同的 url 必然位于对应的文件中，我们只需要统计对应文件中的相同 url 即可
2. 统计：将一个文件存入 hash-set，另一个去查询即可

#### 怎么在海量数据中找出重复次数最多的一个？

1. 分治 + 映射：将大文件分割为小文件
2. 统计：求出每个小文件中重复次数最多的一个，并记录重复次数
3. 排序：找出重复次数最多的一个即可

#### 上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据

1. **上千万或上亿的数据，现在的机器的内存应该能存下**。所以考虑采用hash_map/搜索二叉树/红黑树等来进行统计次数。
2. 然后利用堆取出前N个出现次数最多的数据

#### 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析

1. 如果文件比较大，无法一次性读入内存，可以采用hash取模的方法，将大文件分解为多个小文件，对于单个小文件利用hash_map统计出每个小文件中10个最常出现的词，然后再进行归并处理，找出最终的10个最常出现的词。
2. 通过hash取模将大文件分解为多个小文件后，除了可以用hash_map统计出每个小文件中10个最常出现的词，也可以用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平准长度），最终同样找出出现最频繁的前10个词（可用堆来实现），时间复杂度是O(n*lg10)。

#### 1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？

- hashmap 或者 tire 都行
- 这里要注意容器操作的性能问题

#### 一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解

首先根据用hash并求模，将文件分解为多个小文件，对于单个文件利用上题的方法求出每个文件件中10个最常出现的词。然后再进行归并处理，找出最终的10个最常出现的词

#### 100w 个数中找出最大的 100 个数

- 方案1：采用局部淘汰法。选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中最小的元素比，如果比这个最小的要大，那么把这个最小的元素删除，并把x利用插入排序的思想，插入到序列L中。依次循环，知道扫描了所有的元素。复杂度为O(100w*100)。*
- *方案2：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比100多的时候，采用传统排序算法排序，取前100个。复杂度为O(100w*100)。
- 方案3：在前面的题中，我们已经提到了，用一个含100个元素的最小堆完成。复杂度为O(100w*lg100)。

## 方案二：多层划分

本质上还是分治的思想，重点在分：

- 适用于：第 k 大，中位数，不重复或者重复的数字
- 原理：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行

#### 2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数

- 整数的个数为 2^32 个，最大为 42 亿，因此我们可以将这 2^32 个数划分为 2^8 个区域，将数据分布到不同的区域
- 在各自的区域内使用 bitmap 查重即可

#### 5亿个int找它们的中位数

1. 首先将 int 的空间划分为 2^16 个区域，然后统计各个区域中数的个数，就能得到中位数位于哪一个区域中，并得到中位数在该区域中的位置
2. 在该区域中使用快速排序得到中位数即可

> 对于 int64 的情况，可以进行三次划分，先划分为 2^24 个区域，然后确定命中的区域，接着对该区域划分为 2^20 个子区域，计算第二次命中的区域，此时该区域里只有最多 2^20 个数，直接统计即可

另一种思路：借鉴了基数排序

> 思路二@绿色夹克衫：同样需要做两遍统计，如果数据存在硬盘上，就需要读取2次。
> 方法同基数排序有些像，开一个大小为65536的Int数组，第一遍读取，统计Int32的高16位的情况，也就是0-65535，都算作0,65536 - 131071都算作1。就相当于用该数除以65536。Int32 除以 65536的结果不会超过65536种情况，因此开一个长度为65536的数组计数就可以。每读取一个数，数组中对应的计数+1，考虑有负数的情况，需要将结果加32768后，记录在相应的数组内。
> 第一遍统计之后，遍历数组，逐个累加统计，看中位数处于哪个区间，比如处于区间k，那么0- k-1的区间里数字的数量sum应该<n/2（2.5亿）。而k+1 - 65535的计数和也<n/2，第二遍统计同上面的方法类似，但这次只统计处于区间k的情况，也就是说(x / 65536) + 32768 = k。统计只统计低16位的情况。并且利用刚才统计的sum，比如sum = 2.49亿，那么现在就是要在低16位里面找100万个数(2.5亿-2.49亿)。这次计数之后，再统计一下，看中位数所处的区间，最后将高位和低位组合一下就是结果了。

## 方案三：Bloom filter / bitmap

### Bloom filter 的方案

#### 给你A,B两个文件，各存放50亿条URL，每条URL占用64字节，内存限制是4G，让你找出A,B文件共同的URL。如果是三个乃至n个文件呢？

### bitmap 的方案

#### 在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数

采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可

####  给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？

- 40 亿 个数也就是 40亿 位，已知 10亿 Byte 为 1 GB，则 40 亿 bit 只需要 500 MB 内存即可
- 一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在

## 方案四： Trie / 数据库 / 倒排索引

### Trie

- 适用于：数据量大，重复多，数据种类小
- 基本原理：孩子的表示
- 扩展：Trie 的压缩

> 对于高度重复的场景， Trie 更加节省内存，将 hash 替换为 Trie 即可

### 数据库索引

- 适用于：大量数据的增删改查
- 基本原理：B 树

> 也就是交给数据库去做就好了

### 倒排索引

- 适用于：搜索引擎，关键字查询

这是一种索引方法，用来存储在全文搜索下某个单词在一个文档或者一组文档中存储位置的映射

> 正向索引开发出来用来存储每个文档的单词的列表。正向索引的查询往往满足每个文档有序频繁的全文查询和每个单词在校验文档中的验证这样的查询。在正向索引中，文档占据了中心的位置，每个文档指向了一个它所包含的索引项的序列。也就是说文档指向了它包含的那些单词，而反向索引则是单词指向了包含它的文档，很容易看到这个反向的关系。

## 方案五：外排序（多路归并）

　　适用范围：大数据的排序，去重
　　基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树
问题实例：
　　1).有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16个字节，内存限制大小是1M。返回频数最高的100个词。
　　这个数据具有很明显的特点，词的大小为16个字节，但是内存只有1M做hash明显不够，所以可以用来排序。内存可以当输入缓冲区使用。

### 方案六：分布式处理-MapReduce

MapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。但如果你要我再通俗点介绍，那么，说白了，Mapreduce的原理就是一个归并排序。

适用范围：数据量大，但是数据种类小可以放入内存
基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。
问题实例：

The canonical example application of MapReduce is a process to count the appearances of each different word in a set of documents:
海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。
一共有N个机器，每个机器上有N个数。每个机器最多存O(N)个数并对它们操作。如何找到N^2个数的中数(median)？
    更多具体阐述请参见blog内：

从Hadhoop框架与MapReduce模式中谈海量数据处理，



## 其它模式/方法论，结合操作系统知识

​    至此，六种处理海量数据问题的模式/方法已经阐述完毕。据观察，这方面的面试题无外乎以上一种或其变形，然题目为何取为是：秒杀99%的海量数据处理面试题，而不是100%呢。OK，给读者看最后一道题，如下：
​    非常大的文件，装不进内存。每行一个int类型数据，现在要你随机取100个数。
​    我们发现上述这道题，无论是以上任何一种模式/方法都不好做，那有什么好的别的方法呢？我们可以看看：操作系统内存分页系统设计(说白了，就是映射+建索引)。
​    Windows 2000使用基于分页机制的虚拟内存。每个进程有4GB的虚拟地址空间。基于分页机制，这4GB地址空间的一些部分被映射了物理内存，一些部分映射硬盘上的交换文 件，一些部分什么也没有映射。程序中使用的都是4GB地址空间中的虚拟地址。而访问物理内存，需要使用物理地址。 关于什么是物理地址和虚拟地址，请看：
物理地址 (physical address): 放在寻址总线上的地址。放在寻址总线上，如果是读，电路根据这个地址每位的值就将相应地址的物理内存中的数据放到数据总线中传输。如果是写，电路根据这个 地址每位的值就将相应地址的物理内存中放入数据总线上的内容。物理内存是以字节(8位)为单位编址的。 
虚拟地址 (virtual address): 4G虚拟地址空间中的地址，程序中使用的都是虚拟地址。 使用了分页机制之后，4G的地址空间被分成了固定大小的页，每一页或者被映射到物理内存，或者被映射到硬盘上的交换文件中，或者没有映射任何东西。对于一 般程序来说，4G的地址空间，只有一小部分映射了物理内存，大片大片的部分是没有映射任何东西。物理内存也被分页，来映射地址空间。对于32bit的 Win2k，页的大小是4K字节。CPU用来把虚拟地址转换成物理地址的信息存放在叫做页目录和页表的结构里。 
​    物理内存分页，一个物理页的大小为4K字节，第0个物理页从物理地址 0x00000000 处开始。由于页的大小为4KB，就是0x1000字节，所以第1页从物理地址 0x00001000 处开始。第2页从物理地址 0x00002000 处开始。可以看到由于页的大小是4KB，所以只需要32bit的地址中高20bit来寻址物理页。 
​    返回上面我们的题目：非常大的文件，装不进内存。每行一个int类型数据，现在要你随机取100个数。针对此题，我们可以借鉴上述操作系统中内存分页的设计方法，做出如下解决方案：
​    操作系统中的方法，先生成4G的地址表，在把这个表划分为小的4M的小文件做个索引，二级索引。30位前十位表示第几个4M文件，后20位表示在这个4M文件的第几个，等等，基于key value来设计存储，用key来建索引。

    但如果现在只有10000个数，然后怎么去随机从这一万个数里面随机取100个数？请读者思考。更多海里数据处理面试题，请参见此文第一部分：

## 参考资料

1. https://blog.csdn.net/v_july_v/article/details/7382693
2. https://blog.csdn.net/v_july_v/article/category/774945
3. https://blog.csdn.net/v_july_v/article/category/774945
4. https://blog.csdn.net/v_JULY_v/article/details/6403777
5. https://blog.csdn.net/v_JULY_v/article/details/6279498
6. https://blog.csdn.net/v_july_v/article/details/6685962
7. [https://itimetraveler.github.io/2017/07/13/%E3%80%90%E7%AE%97%E6%B3%95%E3%80%9110%E4%BA%BFint%E5%9E%8B%E6%95%B0%EF%BC%8C%E7%BB%9F%E8%AE%A1%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0/](https://itimetraveler.github.io/2017/07/13/[算法]10亿int型数，统计只出现一次的数/)

